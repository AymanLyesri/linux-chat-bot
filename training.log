INFO     | 2024-04-12 16:34:17 | autotrain.cli.run_llm:run:329 - Running LLM
WARNING  | 2024-04-12 16:34:17 | autotrain.trainers.common:__init__:174 - Parameters supplied but not used: inference, version, train, deploy, func, backend
INFO     | 2024-04-12 16:34:17 | autotrain.backend:create:300 - Starting local training...
INFO     | 2024-04-12 16:34:17 | autotrain.commands:launch_command:338 - ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'bruh1/training_params.json']
INFO     | 2024-04-12 16:34:17 | autotrain.commands:launch_command:339 - {'model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'project_name': 'bruh1', 'data_path': 'bruh1/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': -1, 'model_max_length': 1024, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'save_strategy': 'epoch', 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 15, 'batch_size': 4, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': None, 'target_modules': None, 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': False, 'repo_id': None, 'username': None, 'token': None}
INFO     | 2024-04-12 16:34:26 | __main__:process_input_data:76 - loading dataset from disk
INFO     | 2024-04-12 16:34:26 | __main__:process_input_data:117 - Train data: Dataset({
    features: ['input', 'output', 'autotrain_text'],
    num_rows: 681
})
INFO     | 2024-04-12 16:34:26 | __main__:process_input_data:118 - Valid data: None
INFO     | 2024-04-12 16:34:28 | __main__:train:206 - creating training arguments...
INFO     | 2024-04-12 16:34:28 | __main__:train:220 - Logging steps: 25
INFO     | 2024-04-12 16:34:28 | __main__:train:262 - loading model config...
INFO     | 2024-04-12 16:34:29 | __main__:train:274 - loading model...
INFO     | 2024-04-12 16:34:32 | __main__:train:342 - model dtype: torch.float32
INFO     | 2024-04-12 16:34:32 | __main__:train:350 - preparing peft model...
WARNING  | 2024-04-12 16:34:32 | __main__:train:386 - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.
INFO     | 2024-04-12 16:34:32 | __main__:train:401 - Using block size 1024
INFO     | 2024-04-12 16:34:32 | __main__:train:463 - creating trainer
INFO     | 2024-04-12 16:34:38 | autotrain.trainers.common:on_train_begin:214 - Starting to train...
INFO     | 2024-04-12 16:37:28 | autotrain.trainers.common:on_log:209 - {'loss': 0.9133, 'grad_norm': 0.6409605741500854, 'learning_rate': 0.00017021276595744682, 'epoch': 3.57}
INFO     | 2024-04-12 16:39:58 | autotrain.trainers.common:on_log:209 - {'loss': 0.5882, 'grad_norm': 0.6000972390174866, 'learning_rate': 0.00011702127659574468, 'epoch': 7.14}
INFO     | 2024-04-12 16:42:32 | autotrain.trainers.common:on_log:209 - {'loss': 0.3921, 'grad_norm': 1.060459017753601, 'learning_rate': 6.382978723404256e-05, 'epoch': 10.71}
INFO     | 2024-04-12 16:45:03 | autotrain.trainers.common:on_log:209 - {'loss': 0.3094, 'grad_norm': 0.6880553960800171, 'learning_rate': 1.0638297872340426e-05, 'epoch': 14.29}
INFO     | 2024-04-12 16:45:36 | autotrain.trainers.common:on_log:209 - {'train_runtime': 657.2361, 'train_samples_per_second': 0.593, 'train_steps_per_second': 0.16, 'train_loss': 0.5388358763286045, 'epoch': 15.0}
INFO     | 2024-04-12 16:45:36 | __main__:train:552 - Finished training, saving model...
